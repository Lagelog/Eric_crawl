# Eric爬虫助手

一个功能强大的网页爬虫工具，支持动态内容加载和iframe内容提取。特别优化了对新浪等新闻网站的内容抓取。

## 功能特点

- 支持动态网页内容抓取
- 自动提取iframe中的内容
- 针对新浪等新闻网站进行了特殊优化
- 实时日志显示
- 支持Markdown格式导出
- 美观的Web界面
- 支持需要登录的网站

## 项目结构

```
crawl4ai/
├── app.py              # 主应用入口
├── config.py           # 配置文件
├── logger.py           # 日志处理模块
├── utils.py            # 工具函数模块
├── crawler.py          # 爬虫核心模块
├── routes.py           # 路由处理模块
├── templates/          # 模板文件目录
│   └── index.html      # 主页面模板
├── static/             # 静态文件目录
│   └── style.css       # 样式文件
└── downloads/          # 下载文件保存目录
```

## 依赖要求

- Python 3.7+
- Flask
- Flask-CORS
- pyppeteer
- beautifulsoup4
- markdown
- aiohttp

## 安装

1. 克隆仓库：
```bash
git clone https://github.com/yourusername/crawl4ai.git
cd crawl4ai
```

2. 安装依赖：
```bash
pip install -r requirements.txt
```

## 配置

主要配置项在 `config.py` 文件中：

- `HOST`: 服务器主机地址
- `PORT`: 服务器端口
- `DEBUG`: 调试模式开关
- `DEFAULT_TIMEOUT`: 请求超时时间
- `PAGE_LOAD_WAIT`: 页面加载等待时间
- `IFRAME_LOAD_WAIT`: iframe加载等待时间

## 使用方法

1. 启动服务器：
```bash
python app.py
```

2. 打开浏览器访问：`http://localhost:5000`

3. 在输入框中输入要爬取的网址

4. 如果需要登录，勾选"需要登录"选项

5. 点击"开始爬取"按钮

6. 等待爬取完成，可以查看结果或下载Markdown文件

## 模块说明

### app.py
主应用入口，负责创建和配置Flask应用。

### config.py
集中管理所有配置项，包括路径配置、服务器配置、爬虫配置等。

### logger.py
处理日志记录和订阅，支持实时日志推送。

### utils.py
提供通用工具函数，如Markdown转HTML、异步延迟等。

### crawler.py
爬虫核心模块，实现了网页内容抓取、iframe内容提取等功能。

### routes.py
处理所有Web路由，包括首页、爬取、下载和日志流等接口。

## 特别说明

1. 对于新浪新闻等网站，系统会自动使用特殊的处理方式，以提取更完整的内容。

2. iframe内容提取支持多种方式：
   - 直接访问iframe URL
   - 使用JavaScript提取
   - 备用内容提取方案

3. 支持自动检测和使用本地Chrome/Chromium浏览器。

## 注意事项

1. 确保系统已安装Chrome或Chromium浏览器。

2. 部分网站可能有反爬虫措施，建议合理设置等待时间。

3. 需要登录的网站会打开浏览器供手动登录。

## 错误处理

常见错误及解决方案：

1. 浏览器启动失败
   - 检查Chrome/Chromium是否正确安装
   - 尝试使用有头模式或最小配置模式

2. 内容提取不完整
   - 增加页面加载等待时间
   - 检查网站是否有特殊的加载机制

3. iframe内容获取失败
   - 检查是否存在跨域限制
   - 尝试使用备用提取方案

## 基本原理

### 1. 核心技术栈
- **前端**: Flask Web框架提供用户界面
- **后端**: Python异步编程(asyncio)处理爬虫任务
- **浏览器自动化**: 使用pyppeteer(Python版Puppeteer)实现网页渲染和内容提取
- **内容解析**: 使用BeautifulSoup4进行HTML解析

### 2. 关键实现机制

#### 2.1 浏览器自动化
- 支持多种浏览器启动模式（无头模式、有头模式、最小化配置）
- 自动检测并使用本地Chrome/Chromium浏览器
- 实现了浏览器指纹伪装和反自动化检测

#### 2.2 动态内容处理
- 等待页面完全加载（包括JavaScript渲染的内容）
- 支持动态等待时间，避免内容加载不完整
- 处理AJAX请求和动态加载的内容

#### 2.3 iframe内容提取
实现了多层次的iframe内容提取策略：
1. **直接访问策略**
   - 获取iframe的URL并创建新标签页访问
   - 使用独立的页面上下文获取内容
   
2. **JavaScript注入策略**
   - 使用evaluate方法执行JavaScript代码
   - 直接从DOM中提取iframe内容

3. **可见性检测**
   - 检查iframe元素的实际显示状态
   - 过滤掉不可见或无效的iframe

#### 2.4 特殊网站优化
以新浪网站为例：
- 使用特定的请求头和User-Agent
- 实现了针对性的内容提取策略
- 处理多层嵌套的iframe结构

#### 2.5 登录处理机制
- 支持手动登录模式
- 自动保存和使用登录状态
- 处理验证码和安全检查

#### 2.6 异步处理流程
```
初始化请求 -> 启动浏览器 -> 创建页面 -> 设置请求参数
    -> 访问目标URL -> 等待页面加载 -> 提取主要内容
    -> 处理iframe -> 合并内容 -> 保存结果
```

### 3. 反爬虫策略

#### 3.1 请求伪装
- 随机User-Agent轮换
- 自定义请求头
- 模拟真实浏览器行为

#### 3.2 访问控制
- 随机延迟请求
- 自动处理重定向
- 错误重试机制

#### 3.3 浏览器伪装
- 禁用自动化特征
- 模拟真实浏览器指纹
- 支持代理设置

### 4. 数据处理流程

1. **内容获取**
   - 主页面内容抓取
   - iframe内容提取
   - 动态内容加载

2. **内容解析**
   - HTML解析和清洗
   - 主要内容区域识别
   - 特殊元素处理

3. **数据存储**
   - Markdown格式转换
   - 文件保存
   - 结果展示

