# Eric爬虫助手

一个功能强大的Web爬虫工具，支持动态内容加载、iframe提取和智能登录。

## 项目特点

- 支持动态JavaScript渲染内容的爬取
- 智能处理iframe嵌套内容
- 自动/手动登录支持
- 自适应反爬虫策略
- 高度可配置的爬取选项
- 美观的Web界面
- 实时日志显示
- Markdown格式输出
- 文件自动保存和管理

## 技术栈

- **后端框架**: Flask
- **浏览器自动化**: pyppeteer
- **HTML解析**: BeautifulSoup4
- **前端框架**: 原生JavaScript + CSS3
- **数据格式**: JSON, Markdown
- **异步处理**: asyncio

## 项目结构

```
Eric爬虫助手/
├── app.py              # Flask应用入口
├── config.py           # 配置管理模块
├── crawler.py          # 核心爬虫模块
├── login.py            # 登录管理模块
├── routes.py           # 路由处理模块
├── utils.py            # 工具函数模块
├── logger.py           # 日志管理模块
├── main.py             # 命令行入口
├── templates/          # 前端模板
├── static/             # 静态资源
├── downloads/          # 下载文件存储
├── logs/              # 日志文件存储
└── config/            # 配置文件存储
```

## 模块功能说明

### 1. app.py - 应用入口
- 创建和配置Flask应用实例
- 注册路由蓝图
- 配置CORS和静态文件路径
- 启动Web服务器

### 2. config.py - 配置管理
- 管理项目所有配置项
- 支持环境变量覆盖
- 配置持久化和动态加载
- 提供配置访问接口
- 包含以下主要配置：
  * 路径配置
  * 服务器配置
  * 爬虫配置
  * 浏览器配置
  * 文件管理配置
  * 站点特定配置

### 3. crawler.py - 核心爬虫
- 实现WebCrawler类，提供核心爬取功能
- 支持以下特性：
  * 动态内容加载
  * iframe内容提取
  * 链接提取和分析
  * 递归爬取
  * 内容转换
  * 资源管理
- 实现反爬虫策略：
  * 随机延迟
  * User-Agent轮换
  * 代理支持
  * Cookie管理

### 4. login.py - 登录管理
- 实现LoginManager类，处理网站登录
- 支持功能：
  * 自动登录
  * 手动登录引导
  * 验证码处理
  * Cookie持久化
  * 会话管理
  * 登录状态检测
- 安全特性：
  * 密码加密存储
  * 会话超时处理
  * 异常处理和恢复

### 5. routes.py - 路由处理
- 实现Web接口路由
- 主要接口：
  * 首页渲染
  * 爬取请求处理
  * 文件下载
  * 预览生成
  * 日志流
  * 设置管理
- 特性：
  * 异步处理
  * 错误处理
  * 状态反馈
  * 进度显示

### 6. utils.py - 工具函数
- 提供通用工具函数：
  * Markdown转换
  * 文件处理
  * URL处理
  * 内容提取
  * 格式转换
  * 异步辅助函数

### 7. logger.py - 日志管理
- 实现日志系统：
  * 多级别日志
  * 日志持久化
  * 实时日志流
  * 日志订阅
  * 错误追踪
  * 统计功能

## 实现原理

### 爬虫核心原理
1. **动态内容处理**
   - 使用pyppeteer模拟浏览器行为
   - 等待页面加载完成
   - 执行JavaScript渲染
   - 提取动态生成的内容

2. **智能登录**
   - 自动检测登录表单
   - 模拟用户输入
   - 处理验证码
   - 维护登录会话

3. **内容提取**
   - 选择器匹配
   - 递归处理
   - 智能分析
   - 格式转换

4. **反爬虫策略**
   - 动态User-Agent
   - 随机延迟
   - 代理切换
   - 请求限制

### 数据处理流程
1. URL解析和验证
2. 浏览器实例创建
3. 页面加载和渲染
4. 内容提取和处理
5. 数据清洗和转换
6. 结果保存和输出

## 使用说明

### 安装依赖
```bash
pip install -r requirements.txt
```

### 启动服务
```bash
python app.py
```

### 配置说明
1. 基础配置在config.py中
2. 自定义配置可在config/custom_config.json中设置
3. 环境变量可覆盖默认配置

### 使用流程
1. 访问Web界面
2. 输入目标URL
3. 配置爬取选项
4. 启动爬取
5. 查看结果和下载

## 注意事项

1. 确保安装了Chrome/Chromium浏览器
2. 遵守目标网站的robots.txt规则
3. 合理设置爬取间隔
4. 定期清理日志和下载文件
5. 注意内存和CPU使用


